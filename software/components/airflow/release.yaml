---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: airflow
  namespace: flux-system
  labels:
    name: airflow
spec:
  targetNamespace: airflow
  releaseName: airflow
  dependsOn:
    - name: azure-keyvault-airflow
      namespace: default
  install:
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 10
      strategy: rollback
  test:
    enable: true
  interval: 10m0s
  chart:
    spec:
      chart: airflow
      # version: 1.11.0
      sourceRef:
        kind: HelmRepository
        name: airflow
        namespace: flux-system
  values:

    # Version Isolation
    # airflowVersion: 2.10.1
    # defaultAirflowTag: 2.10.1

    # Airflow executor
    executor: KubernetesExecutor

    # Environment variables for all airflow containers
    env:
      - name: ENVIRONMENT
        value: "dev"
      - name: CLOUD_PROVIDER
        value: "azure"
      - name: CI_COMMIT_TAG
        value: "v0.12.0"
      - name: BUILD_TAG
        value: "v0.12.0"
      - name: PYTHONPATH
        value: "/opt/celery"
      - name: AIRFLOW_VAR_ENTITLEMENTS_MODULE_NAME
        value: "entitlements_client"

    extraEnv: |
      - name: AIRFLOW_VAR_ANOTHER_KEY
        value: 'value_1'
      
    extraConfigMaps:
      'airflow-variables':
        data: |
          AIRFLOW__SCHEDULER__STATSD_ON: "True"
          AIRFLOW__SCHEDULER__STATSD_HOST: "appinsights-statsd"
          AIRFLOW__SCHEDULER__STATSD_PORT: 8125
          AIRFLOW__SCHEDULER__STATSD_PREFIX: "osdu_airflow"
          AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "False"
          ## Enable for Debug purpose
          AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "False"
          AIRFLOW__WEBSERVER__AUTHENTICATE: "True"
          AIRFLOW__WEBSERVER__AUTH_BACKEND: "airflow.contrib.auth.backends.password_auth"
          AIRFLOW__WEBSERVER__RBAC: "True"
          AIRFLOW__API__AUTH_BACKEND: "airflow.api.auth.backend.default"
          AIRFLOW__CORE__REMOTE_LOGGING: "True"
          AIRFLOW__CORE__REMOTE_LOG_CONN_ID: "az_log"
          AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER: "wasb-airflowlog"
          AIRFLOW__CORE__LOGGING_CONFIG_CLASS: "log_config.DEFAULT_LOGGING_CONFIG"
          AIRFLOW__CORE__LOG_FILENAME_TEMPLATE: "{{ run_id }}/{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{% if dag_run.conf is not none and 'correlation_id' in dag_run.conf %}{{ dag_run.conf['correlation_id'] }}{% else %}None{% endif %}/{{ try_number }}.log"
          AIRFLOW__CELERY__SSL_ACTIVE: "True"
          AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: "True"
          AIRFLOW__CORE__PLUGINS_FOLDER: "/opt/airflow/plugins"
          AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 60
          AIRFLOW__CORE__LOGGING_LEVEL: DEBUG
          AIRFLOW_VAR_CORE__CONFIG__DATALOAD_CONFIG_PATH: "/opt/airflow/dags/configs/dataload.ini"
          AIRFLOW__WEBSERVER__WORKERS: 8
          AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE: 0
          AIRFLOW__CORE__STORE_SERIALIZED_DAGS: True #This flag decides whether to serialise DAGs and persist them in DB
          AIRFLOW__CORE__STORE_DAG_CODE: True #This flag decides whether to persist DAG files code in DB
          AIRFLOW__WEBSERVER__WORKER_CLASS: gevent
          AIRFLOW__CELERY__WORKER_CONCURRENCY: 16 # Do not remove this config as it is used for autoscaling as well
          AIRFLOW_VAR_CORE__CONFIG__SHOW_SKIPPED_IDS: True


      'airflow-service-endpoints':
        data: |
          AIRFLOW_VAR_CORE__SERVICE__PARTITION__URL: "http://partition.osdu-core.svc.cluster.local/api/partition/v1"
          AIRFLOW_VAR_CORE__SERVICE__LEGAL__HOST: "http://legal.osdu-core.svc.cluster.local/api/legal/v1"
          AIRFLOW_VAR_CORE__SERVICE__ENTITLEMENTS__URL: "http://entitlements.osdu-core.svc.cluster.local/api/entitlements/v2"
          AIRFLOW_VAR_CORE__SERVICE__SCHEMA__URL: "http://schema.osdu-core.svc.cluster.local/api/schema-service/v1"
          AIRFLOW_VAR_CORE__SERVICE__SEARCH__URL: "http://search.osdu-core.svc.cluster.local/api/search/v2"
          AIRFLOW_VAR_CORE__SERVICE__SEARCH_WITH_CURSOR__URL: "http://search.osdu-core.svc.cluster.local/api/search/v2/query_with_cursor"
          AIRFLOW_VAR_CORE__SERVICE__STORAGE__URL: "http://storage.osdu-core.svc.cluster.local/api/storage/v2"
          AIRFLOW_VAR_CORE__SERVICE__FILE__HOST: "http://file.osdu-core.svc.cluster.local/api/file"
          AIRFLOW_VAR_CORE__SERVICE__WORKFLOW__HOST: "http://workflow.osdu-core.svc.cluster.local/api/workflow/v1"
          AIRFLOW_VAR_CORE__SERVICE__DATASET__HOST: "http://dataset.osdu-core.svc.cluster.local/api/dataset/v1"


    extraEnvFrom: |
      - configMapRef:
          name: "airflow-variables"
      - configMapRef:
          name: "airflow-service-endpoints"
      - secretRef:
          name: 'airflow-variables'

    # Modify Create User Job command to use secrets
    # Airflow create user job settings
    createUserJob:
      useHelmHooks: false
      # Limit the lifetime of the job object after it finished execution.
      ttlSecondsAfterFinished: 300
      # Command to use when running the create user job (templated).
      command: ~
      # Args to use when running the create user job (templated).
      args:
        - "bash"
        - "-c"
        # The format below is necessary to get `helm lint` happy
        - |-
          exec \
          airflow {{ semverCompare ">=2.0.0" .Values.airflowVersion | ternary "users create" "create_user" }} "$@"
        - --
        - "-r"
        - "{{ .Values.webserver.defaultUser.role }}"
        - "-u"
        - "$(ADMIN_USERNAME)"
        - "-e"
        - "{{ .Values.webserver.defaultUser.email }}"
        - "-f"
        - "{{ .Values.webserver.defaultUser.firstName }}"
        - "-l"
        - "{{ .Values.webserver.defaultUser.lastName }}"
        - "-p"
        - "$(ADMIN_PASSWORD)"
      env:
        - name: ADMIN_USERNAME
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: username
        - name: ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: password

    # Admin user configuration
    admin:
      existingSecret: airflow-secrets
      usernameKey: username
      passwordKey: password

    # Disable the internal PostgreSQL chart
    postgresql:
      enabled: false

    # External PostgreSQL configuration
    data:
      metadataSecretName: airflow-secrets

    # Disable pgbouncer.
    # CloudNativePG provides native support for connection pooling with PgBouncer
    pgbouncer:
      enabled: false

    # StatsD settings
    statsd:
      enabled: true

    # Pull DAGS from file share and from a git repo
    dags:
      persistence:
        enabled: true
        existingClaim: airflow-dags-pvc
      gitSync:
        enabled: false

    logs:
      persistence:
        enabled: true
        existingClaim: airflow-logs-pvc

    config:
      webserver:
        expose_config: 'True'
        instance_name: OSDU
        enable_proxy_fix: 'True'
        # base_url: 'http://localhost/airflow'
      operators:
        default_owner: OSDU

    # We disable the log groomer sidecars because we use Azure File Storage for logs
    triggerer:
      logGroomerSidecar:
        enabled: false
    scheduler:
      logGroomerSidecar:
        enabled: false
    workers:
      logGroomerSidecar:
        enabled: false
    webserver:
      defaultUser:
        enabled: true
        role: Admin
        email: admin@example.com
        firstName: admin
        lastName: user

    # Ensure we are using the secrets from Azure Key Vault
    enableBuiltInSecretEnvVars:
      AIRFLOW__CORE__FERNET_KEY: false
      AIRFLOW__WEBSERVER__SECRET_KEY: false
    fernetKeySecretName: keyvault-secrets
    webserverSecretKeySecretName: keyvault-secrets

    # Airflow database migration job settings
    migrateDatabaseJob:
      useHelmHooks: false

    # Ensure this goes on the correct node pool.
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: agentpool
              operator: In
              values:
                - poolz1
                - poolz2
                - poolz3  # Adjust these values to match your node pools

    topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
      labelSelector:
        matchLabels:
          release: airflow  # Adjust to match your Helm release name

    # Tolerations should be defined outside of affinity
    tolerations:
      - effect: NoSchedule
        key: app
        value: "cluster"
